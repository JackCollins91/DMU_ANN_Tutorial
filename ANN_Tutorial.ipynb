{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Hands-On Introduction to Artificial Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By: [John 'Jack' Collins](https://www.mzes.uni-mannheim.de/d7/en/profiles/john-james-collins)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are powerful machine learning algorithms that form the basis of many important technologies, including generative AI and computer vision. However, they're not as straight-forward to implement as many other machine learning techniques, like random forest or logistic regression. If you're a researcher interested in applying neural networks, we've made a handy guide to help you easily prototype a neural network for your own use-case.\n",
    "\n",
    "In this [tutorial](https://www.mzes.uni-mannheim.de/socialsciencedatalab/), we demonstrate how to get started with Artificial Neural Networks (ANN). This article is aimed at researchers who may be familiar with python, know some basic machine learning (i.e. logistic regression or random forest), but have yet to try using neural networks.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Demonstrate how to quickly produce a model, train, validate, and evaluate it. We also show how to efficiently find the best hyperparameters.\n",
    "- Offer reuseable code which makes it easy to prototype ANNs in your own project. \n",
    "- Introduce you to libraries Keras and Tensorflow, which together make one of the most popular approaches to AI coding. \n",
    "- Finish by demonstrating how a Keras model can be saved and loaded, enabling you to deploy the model as you please. \n",
    "\n",
    "By way of example, we will step through a simple classification problem with the well-known 'iris' dataset. We close with a brief demonstration of how the same code can easily be re-used to accomplish other use cases as well. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### What are ANNs?\n",
    "Even if you have no familiarity with the theory, you can still follow this tutorial, just review the following key points.\n",
    "\n",
    "- An Artificial Neural Network is a type of supervised machine learner. It is trained on data and can then be used to make predictions about new data. In this tutorial, we will train an ANN to classify species of flowers based on measurements of their petals. \n",
    "\n",
    "- In this tutorial, we introduce the simplest kind of ANN, a 'Feed Forward Neural Network' (FNN), also known as a 'Multilayer Perceptron' (MLP). There are many more complex types of ANN which are better-suited to more complex tasks, like time-series prediction (with Recurrent Neural Networks) and Image Processing (with Convolutional Neural Networks). These are great techniques to learn, and this tutorial offers a solid basis from which to continue learning about those elsewhere. In fact, you may even reuse the same code with minor adjustments to perform those exact techniques. \n",
    "\n",
    "- This tutorial is about stepping the reader through an example, and not about explaining what ANNs are. However, if you want to understand the theory, there are many resources available: We suggest [this video series ](https://www.youtube.com/watch?v=CqOfi41LfDw&ab_channel=StatQuestwithJoshStarmer) for an introduction to the theory of ANNs. If you want a very detailed understanding and are willing to purchase it, we can also recommend this excellent [Udemy course](https://www.udemy.com/course/deep-learning-tensorflow-2/).\n",
    "\n",
    "- However, here's a brief description of an FNN: An FNN is composed of computational units called 'neurons' which are grouped in sets called 'layers.' The inputs from the independent features are passed through the layers of neurons. The neurons fit weights to the inputs in a way that derives connections between the independent features and the dependant variable.\n",
    "\n",
    "- While ANNs are very well-suited for certain machine learning problems, in many scenarios simpler models, like logistic regression, may actually be better than ANNs. We'll explain this in the next section.\n",
    "\n",
    "\n",
    "### What are ANNs useful for?\n",
    "\n",
    "ANNs excel at non-linearly separable problems. In the following figure, we can see an example of a non-linearly separable problem compared to one that is linear. \n",
    "\n",
    "![linear-problems alt ><](fig1.png)\n",
    "\n",
    "With a linear problem, you can see how it would be possible to draw a line (straight or curvy) between the blue and red dots. This means we could derive a linear function (or a polynomial function) which takes the X and Y coordinates and outputs a classification for whether the dot is blue or red. But in the non-linear problem, there is no way a line (no matter how curvy) can separate the blue and red dots. \n",
    "\n",
    "The functions which underpins ANNs allows for interactions between variables. This means the underlying function can fit to the X and Y inputs such that a dot is classified blue if the X coordinate is of a certain value AND the Y coordinates are within a certain range also. This innovation means that ANNs can fit to types of problems that are impossible for linear models. \n",
    "\n",
    "However, ANNs are not always best. Because ANNs can fit so finely to data, it is vulnerable to overfitting. When a problem is linearly separable, it may often be better to use the simpler, and less overfit-prone model instead. \n",
    "\n",
    "Those readers familiar with tree-based models might recognize that tree algorithms can also handle interactions between features. It is true that tree-based models are also good for non-linearly separable problems as well as ANNs. Tree-based models partition a feature space, such as the X and Y coordinate space in the diagram above, into subsections and classify points based on which subsection they fall into. A key difference betweens ANNs and trees, is that while tree-based models usually have distinct boundaries between subsections (a result of the binary decision functions in the trees), ANNs are better suited to yield varying probabilities for classification classes across the space. This is not to say tree-based models do not output probabilistic predictions (they do), just that ANNs can be better suited to finely tune those probabilities. See [this article](https://towardsdatascience.com/when-and-why-tree-based-models-often-outperform-neural-networks-ceba9ecd0fd8#:~:text=The%20primary%20difference%20in%20usage,power%20of%20tree%2Dbased%20methods.) for a deeper discussion of the differences between tree-based models and neural networks. \n",
    "\n",
    "\n",
    "### How do I get started?\n",
    "\n",
    "In this tutorial, we will step through an example from which you can reuse this code in your own project. We will demonstrate how to use Keras and Tensorflow, together, this is the most common starting point for learning neural networks. Before we begin, here's a quick introduction to these two packages.\n",
    "\n",
    "#### What is Tensorflow?\n",
    "\n",
    "[Tensorflow](https://www.tensorflow.org/about#:~:text=TensorFlow%20gives%20you%20the%20flexibility,fast%20debugging%2C%20use%20eager%20execution.) is one of [the most popular](https://www.forbes.com/sites/janakirammsv/2020/11/27/tensorflow-turns-5five-reasons-why-it-is-the-most-popular-ml-framework/) tools for developing neural networks. Tensorflow is not a library for making neural networks per se. Rather, Tensorflow is for efficiently handling the mathematic operations that neural networks rely on. ANNs utilize a lot of linear algebra, matrix operations and node-edge graph manipulations. Tensorflow provides a set of functionality to accomplish these operations efficiently with C++ code and allows the developer to use these functions with a python interface. \n",
    "\n",
    "#### What is Keras?\n",
    "As discussed, Tensorflow itself doesn't directly provide neural network functionality. Instead, the programmer who wants to develop an ANN will need another library. [Keras](https://keras.io/why_keras/#:~:text=Keras%20prioritizes%20developer%20experience&text=Keras%20follows%20best%20practices%20for,learn%20and%20easy%20to%20use.) provides programmers a way to declare what kind of ANN they want and then Keras accomplishes the interfacing with Tensorflow to generate the network. Tensorflow focusses on accomplishing the mathematics efficiently, while Keras provides programmers an easy way to develop their models. \n",
    "Although Keras and Tensorflow are very often used together, they are actually independent. Keras can be used to interface with an alternative to Tensorflow, like Theano. Similarly, developers can use an alternatives to Keras, like TFLearn and still use Tensorflow in the backend. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before writing the code, we import the necessary libraries and set a few configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\utils\\_clustering.py:35: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _pt_shuffle_rec(i, indexes, index_mask, partition_tree, M, pos):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\utils\\_clustering.py:54: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def delta_minimization_order(all_masks, max_swap_size=100, num_passes=2):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\utils\\_clustering.py:63: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window(order, start, length):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\utils\\_clustering.py:69: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _reverse_window_score_gain(masks, order, start, length):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\utils\\_clustering.py:77: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _mask_delta_score(m1, m2):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\links.py:5: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def identity(x):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\links.py:10: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _identity_inverse(x):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\links.py:15: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def logit(x):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\links.py:20: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _logit_inverse(x):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\utils\\_masked_model.py:363: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_single_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\utils\\_masked_model.py:385: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _build_fixed_multi_output(averaged_outs, last_outs, outputs, batch_positions, varying_rows, num_varying_rows, link, linearizing_weights):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\utils\\_masked_model.py:428: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _init_masks(cluster_matrix, M, indices_row_pos, indptr):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\utils\\_masked_model.py:439: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _rec_fill_masks(cluster_matrix, indices_row_pos, indptr, indices, M, ind):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\maskers\\_tabular.py:186: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _single_delta_mask(dind, masked_inputs, last_mask, data, x, noop_code):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\maskers\\_tabular.py:197: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _delta_masking(masks, x, curr_delta_inds, varying_rows_out,\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\maskers\\_image.py:175: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def _jit_build_partition_tree(xmin, xmax, ymin, ymax, zmin, zmax, total_ywidth, total_zwidth, M, clustering, q):\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_partition.py:676: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def lower_credit(i, value, M, values, clustering):\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "\u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Libraries to pip install\n",
    "# keras-tuner\n",
    "# matplotlib\n",
    "# numpy\n",
    "# pandas\n",
    "# scikit-learn\n",
    "# tensorflow\n",
    "\n",
    "import copy \n",
    "import keras_tuner as kt # for hypertuning with keras models. \n",
    "import matplotlib # visualization library\n",
    "import numpy as np # popular library for mathematic operations\n",
    "import pandas as pd # popular library for data table manipulation\n",
    "import shap # explained in 'interpretation' section below\n",
    "import shutil \n",
    "import sklearn as sk # for evaluation metrics and the example datasets. \n",
    "import tensorflow as tf \n",
    "import warnings\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay, \n",
    "    accuracy_score, \n",
    "    mean_absolute_error\n",
    "    )\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.losses import Loss\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "\n",
    "## Tensorflow issues some warnings but they are not \n",
    "# important for us, so we'll just suppress them. \n",
    "warnings.simplefilter(\"ignore\")\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "## Setting random seed for reproducible results\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "In this section, we introduce the dataset for our experiment. We use the [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris), in which we aim to classify which of three species of flower the subject is, based on measurements of petals and sepals. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris(return_X_y=False, as_frame=False)\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "## a keras utility function to one hot encode (OHE) the vector of \n",
    "# categories y.\n",
    "y = np_utils.to_categorical(y) \n",
    "\n",
    "## The number of unique classes we seek to classify. \n",
    "# There are three species of iris in our dataset. \n",
    "n_classes = len(dataset.target_names) \n",
    "\n",
    "## ANNs require us to specify an 'input shape.' If our input were \n",
    "# images, we might have a 2d input, but for this problem, \n",
    "# it is just the number of predictive features, which is 4.\n",
    "input_shape = len(dataset.feature_names)\n",
    "\n",
    "## This is a list of the titles of the features.\n",
    "feature_names = dataset.feature_names \n",
    "\n",
    "## Test-train splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.33, \n",
    "    random_state=42\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Inspection\n",
    "\n",
    "#### The predictive features (X)\n",
    "Our X value is a matrix of floats where each row is a flower and each column is a measurement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X,columns = feature_names).head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dependant (y)\n",
    "For each row in X, there is a corresponding item in the vector y, which represents the species of flower. There are three unique species in our dataset as shown in the table below.\n",
    "However, our dependant y must come in the form of a One Hot Encoded (OHE) matrix, not a vector. In this format, there are three columns with a zero or one value to indicate which species of flower is this subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   setosa  versicolor  virginica\n",
       "0     1.0         0.0        0.0\n",
       "1     1.0         0.0        0.0\n",
       "2     1.0         0.0        0.0\n",
       "3     1.0         0.0        0.0\n",
       "4     1.0         0.0        0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y[:5],columns = dataset.target_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us inspect this same data when converted to an array, this will be used to 'flatten' the matrix form of y to a vector. Each row is a 'setosa' (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5].argmax(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple ANN\n",
    "\n",
    "If we want to develop a simple ANN, the Keras library makes this extremely simple.\n",
    "\n",
    "Firstly, we instantiate an empty 'sequential' model, so-called because each layer of neurons (which we shall add shortly) will feed into the subsequent layer (hence why this is also called a 'Feed Forward' network).\n",
    "\n",
    "Next, we must populate our empty Sequential model with layers. Here we will use 'Dense' layers, which are simply arrays of neurons. This is the most basic type of neural network layer. \n",
    "\n",
    "After the Dense layer, we add a 'Dropout' layer. The Dropout layer is like a gatekeeper: it selects certain neurons in the previous layer and blocks its input from going to the next layer. This nullification is helpful for avoiding overfitting caused by neurons over-weighting certain signals in the data. Dropout layers use a fitting process to determine which neurons are the best to 'drop' to improve predictive performance. \n",
    "\n",
    "Finally, we add the output layer. The shape of the output layer is important because it determines the format of the predictions. Because we have three classes in our classification problem, we want three outputs: each one will yield a float which represents the probability that a given case is of the corresponding species. \n",
    "If this were a regression problem, or a binary regression problem, we might only want one output unit, which would output just the regression value, or the probability of the binary case being a positive. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terminology:\n",
    "- Batch size: This parameter instructs the ANN to trial that number of cases with a set of \n",
    "random weights before trialing another batch with a new set of weights. A large batch size enhances computational efficiency, at the potential cost of accuracy. \n",
    "\n",
    "- Epochs: The number of times the ANN shall repeatedly use the training data to find the optimum set of weights. \n",
    "A large number of epochs may increase accuracy, but takes longer to compute. \n",
    "\n",
    "- Optimizer: In the fitting process, random weights and thresholds are trialed for every node and the best combination is selected. However, trying every possible random combination is very inefficient. Instead, optimizers apply algorithms that aim to discover which combinations of parameters for each node is best. Instead of randomly selecting weights, the optimizer applies a guided process to discovering the best settings. These algorithms for determining the optimum weights are called 'gradient descent' algorithms. \n",
    "\n",
    "- Learning rate: The incremental value by which the optimizer varies weights.  A larger learning rate may be quicker to compute, but less accurate.\n",
    "\n",
    "- Loss Function: During the fitting process, the loss function is how we calculate if a given trial was better or worse that others. Here we use 'Binary Cross Entropy' which is a common selection for classification problems. Mean-squared error would be an appropriate choice for regression problems. \n",
    "\n",
    "- Metrics: Machine learning practitioners know there are many ways to evaluate a model's performance. We can record multiple metrics to evaluate performance. For example, we can examine the model's accuracy (the portion of correct predictions) and the recall (the portion of correctly classified cases of a given class among all cases of that class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n",
      "Accuracy of this model is: 70.00%\n"
     ]
    }
   ],
   "source": [
    "def get_simple_ann(\n",
    "        input_shape: int|tuple,\n",
    "        output_shape:int|tuple\n",
    "        ) -> keras.models.Sequential:\n",
    "    \"\"\"\n",
    "    This function returns an instance of a \n",
    "    Keras Sequential model which is ready to \n",
    "    fit with training data.\n",
    "    \n",
    "    Args:\n",
    "    input_shape: int or tuple. Describes the \n",
    "    dimensions of a single case of input.\n",
    "    output_shape: int or tuple. The desired \n",
    "    shape of output. \n",
    "    \n",
    "    Returns:\n",
    "    A compiled instance of a keras Sequential model\n",
    "    \"\"\"\n",
    "    ann = keras.models.Sequential()\n",
    "    \n",
    "    ## Adding the input layer with same number \n",
    "    # of nodes as we have input features\n",
    "    ann.add(\n",
    "        keras.layers.Dense(\n",
    "            units = 16,\n",
    "            activation='relu', \n",
    "            input_dim=input_shape\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Adding 10% dropout to prevent overfitting\n",
    "    ann.add(keras.layers.Dropout(rate=0.1))\n",
    "    \n",
    "    ## Adding another Dense-Dropout combo to \n",
    "    # make a second layer in the network\n",
    "    ann.add(\n",
    "            keras.layers.Dense(\n",
    "            units = 16, \n",
    "            activation='relu'\n",
    "            )\n",
    "        )\n",
    "    ann.add(keras.layers.Dropout(rate=0.1))\n",
    "    \n",
    "    ## Adding the output layer. This must match the \n",
    "    # format of our y variable\n",
    "    ann.add(\n",
    "            keras.layers.Dense(\n",
    "            units = output_shape, \n",
    "            activation='softmax'\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    ## Once we have defined the architecture of \n",
    "    # our ANN, we call compile() to convert the model to tensors.\n",
    "    ann.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    return ann\n",
    "\n",
    "## Calling our function\n",
    "simple_ann = get_simple_ann(\n",
    "    input_shape = input_shape,\n",
    "    output_shape = n_classes\n",
    "    )\n",
    "\n",
    "## Fitting to training data\n",
    "simple_ann.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=100, \n",
    "    epochs=150,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "## Making predictions\n",
    "# argmax() here is used to 'flatten' the matrix form of the \n",
    "# dependant variable (as seen above) into a vector of integers \n",
    "# which correspond to classes. \n",
    "predictions = simple_ann.predict(X_test).argmax(axis=1) \n",
    "\n",
    "## Accuracy score\n",
    "accuracy = accuracy_score(y_test.argmax(axis=1),predictions)\n",
    "print(\"Accuracy of this model is: \"+\"{:1.2f}\".format(accuracy*100)+\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the ANN\n",
    "![image](fig2.png)\n",
    "\n",
    "From the code above, we are constructing an ANN like the one in this diagram. We have four features in our data, and so we have four input nodes to receive each value. If you know the theory of ANNs, you know that each node will apply a weight and an activation function to the input to determine the number it will output to every node in the next layer. However, we use a dropout layer, which will select some nodes to nullify. We have two such layers. Finally, we have three output nodes, representing the probability that the given case is of a certain species. Therefore, for each case we predict, we output three numbers. We conclude that whichever of those three numbers is largest corresponds to the most likely species for the given case. \n",
    "### Evaluating this model\n",
    "As we can see from the output above, this model was 70% accurate, meaning it classified 70% of cases as the correct species. \n",
    "\n",
    "### Why we need hypertuning\n",
    "Is 70% accuracy the best we can do? What if we had more than 12 nodes in each layer? Or fewer? What if we added more layers? Or increased the dropout rate?\n",
    "Hypertuning is how we experimentally verify which parameters are best.\n",
    "\n",
    "To accomplish this, we could try repeatedly fitting the model with every possible combination of parameters (this is called 'Grid Search'). However, for many parameters, this would require a lot of fittings. Instead, the ['keras tuner'](https://keras.io/guides/keras_tuner/getting_started/) library was created to efficiently trial different hyperparameter selections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Hypermodel Class\n",
    "In this section, we provide the code for a hypermodel class. You can copy this class into your own project and try it. If you are familiar with inheritance, you could also extend this class for more complex functionality, such as time-series or computer vision problems. This class provides a solid base from which to trial many different kinds of ANNs and apply it to many kinds of machine learning tasks. You could easily reuse this class for regression, or a different classification problem.\n",
    "The purpose of this class is to:\n",
    "- Receive a dictionary describing the desired hyperparameters to explore\n",
    "- Receive the parameters of how you wish to fit the model (epochs, batch size, etc)\n",
    "- Efficiently discover the optimum hyperparameters\n",
    "- Returns the 'best' discovered model\n",
    "\n",
    "The following is a lot of code, so if you are not interested in the details, you can skip reading the following code block. In the next section, we shall demonstrate the use of the class, which you can go on to reading instead. Like many classes, you don't necessarily need to know all the details of *how* it works, only how to use it. \n",
    "\n",
    "If you would like to understand this class in detail, we recommend reading the documentation on the [keras-tuner library](https://keras.io/guides/keras_tuner/getting_started/).\n",
    "\n",
    "To help explain the class, we also introduce some more terminology.\n",
    "\n",
    "#### Terminology\n",
    "\n",
    "- Objective: One of the metrics. The 'best' model is the one with the highest value. \n",
    "- Hypergrid: Is a dictionary-like structure that specifies what values for each parameter to trial.\n",
    "- Callbacks: In this context, the callback is a special type of function called at the end of each epoch. In this example, we use 'EarlyStopping,' which is helpful for reducing computation time. With early stopping, once each additional epoch only improves the objective slightly, we conclude we have reached a point of diminishing return and finish the fitting process without completing every epoch.\n",
    "- Width: The number of units is a particular layer.\n",
    "- Depth: The number of layers in an ANN.\n",
    "- Validation split: By default, the tuner selects the model with the best value for the objective as calculated on training data. However, we can optionally specify a validation split size, whereby a random sample of training data will be withheld, and we select the best model as validated against that holdout data. This can be helpful in avoiding overfitting to training data.\n",
    "- Activation function: a function which each node in a layer uses to convert the inputted values into the outputted values. A sigmoid function would be appropriate for the activation function of an output node doing classification, while no function would be appropriate for a regression. For a discussion on the different types of common functions, see [this article](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNHyperModel(kt.HyperModel):\n",
    "    \"\"\"\n",
    "    Given a set of hyperparameters to explore, \n",
    "    uses keras-tuner to discover the best settings \n",
    "    and returns the optimum model.\n",
    "    \"\"\"\n",
    "    \n",
    "    _default_hypergrid = {\n",
    "            'width':{\n",
    "                'min':12,\n",
    "                'max':12,\n",
    "                'step':1\n",
    "                },\n",
    "            'depth':{\n",
    "                'min':1,\n",
    "                'max':1,\n",
    "                'step':1\n",
    "                },\n",
    "            'droprate':{\n",
    "                'min':.8,\n",
    "                'max':.8,\n",
    "                'step':.1\n",
    "                },\n",
    "            'activation':{\n",
    "                'values':['relu']\n",
    "            },\n",
    "            'learning_rate':{\n",
    "                'values':[1e-2]\n",
    "            }\n",
    "        }\n",
    "    \"\"\"If the user does not input values for the \n",
    "    following hyperparameters, we use these values by default.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "                self,\n",
    "                input_shape:tuple|int,\n",
    "                n_outputs:int,\n",
    "                hypergrid:dict = None,\n",
    "                output_activation:str = 'sigmoid',\n",
    "                epochs:int = 1,\n",
    "                batch_size:int = 100,\n",
    "                optimizer_class = keras.optimizers.Adam,\n",
    "                loss: Loss = SparseCategoricalCrossentropy(\n",
    "                                        from_logits=True\n",
    "                                        ),\n",
    "                metrics: list[str] = ['accuracy'],\n",
    "                objective:str = \"accuracy\",\n",
    "                directory:str = None,\n",
    "                project_name:str = \"kt_hyperband\",\n",
    "                factor:int = 3,\n",
    "                verbose:int = 0,\n",
    "                callbacks:list = []\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - input_shape (tuple | int): Dimensions of a single input case. \n",
    "        Provide an int of n if one case is a 1-d array of length n.\n",
    "\n",
    "        - n_outputs (int): Number of output nodes. One for regression or \n",
    "        binary-classification. Multiple for \n",
    "        multi-regression/multi-classification.\n",
    "\n",
    "        - hypergrid (dict, optional): Dictionary describing hyperparams \n",
    "        to trial, see default '_default_hypergrid' for example. \n",
    "        Defaults to None.\n",
    "\n",
    "        - output_activation (str, optional): Output layer activation \n",
    "        function. Defaults to 'sigmoid'.\n",
    "\n",
    "        - epochs (int, optional): Training epochs. Defaults to 1.\n",
    "\n",
    "        - batch_size (int, optional): training batch size. \n",
    "        Defaults to 100.\n",
    "\n",
    "        - optimizer_class (_type_, optional): Optimizer class. \n",
    "        Input the class type, not an instantiation. Defaults to \n",
    "        keras.optimizers.Adam.\n",
    "\n",
    "        - loss (tf.keras.losses.Loss, optional): training loss function. \n",
    "        Defaults to \n",
    "        keras.losses.SparseCategoricalCrossentropy(from_logits=True).\n",
    "        \n",
    "        - metrics (list[str], optional): Training metrics. Defaults \n",
    "        to ['accuracy'].\n",
    "        \n",
    "        - objective (str, optional): Must be included in 'metrics.' \n",
    "        Tuning process will select the model with the best performance\n",
    "        by this metric. Defaults to \"accuracy\".\n",
    "        \n",
    "        - directory (str, optional): When used, tuning process creates \n",
    "        a directory to hold data on disk. Deleted afterwards. \n",
    "        Be careful not to use existing dir name. Defaults to \"None\".\n",
    "        \n",
    "        - project_name (str, optional): File naming stem in directory. \n",
    "        Defaults to \"kt_hyperband\".\n",
    "\n",
    "        -factor (int, optional): Reduction factor for epochs at each \n",
    "        tuning trial. Defaults to 3.\n",
    "\n",
    "        -verbose (int, optional): Set to true to print details of \n",
    "        fitting process. Defaults to 0.\n",
    "        \n",
    "        - callbacks (list, optional): A keras.callback function called \n",
    "        after each epoch. Defaults to [].\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.n_outputs = n_outputs\n",
    "        self.hp = kt.HyperParameters()\n",
    "        self.hypergrid = hypergrid\n",
    "        self.output_activation = output_activation\n",
    "        self.epochs = epochs\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        self.objective = objective\n",
    "        self.callbacks = callbacks\n",
    "        self.directory = directory\n",
    "        self.project_name = project_name\n",
    "        self.factor = factor\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.history = None # null before training\n",
    "\n",
    "    def get_hypergrid(self):\n",
    "        \"\"\"Returns the user-specified hypergrid, plus the default items \n",
    "        (from _default_hypergrid) where the user did not specify \n",
    "        values\"\"\"\n",
    "        output = copy.deepcopy(ANNHyperModel._default_hypergrid)\n",
    "        if self.hypergrid is None:\n",
    "             self.hypergrid = {}\n",
    "        output.update(self.hypergrid)\n",
    "        return output\n",
    "\n",
    "    ## If you want to try more complex types of ANN, you should extend \n",
    "    # this class and overwrite this function with your own logic. \n",
    "    # For example, instead of adding Dense layers, you could add LSTM \n",
    "    # layers instead to create an RNN. \n",
    "    def build(self,hp:kt.HyperParameters):\n",
    "        \"\"\"Using an instance of kt.HyperParameters, we build and \n",
    "        compile a Keras Sequential instance with \n",
    "        some parameters filled by HyperParameter objects instead of \n",
    "        literal values. The Keras Hyperband object can then execute \n",
    "        a search function using this hypermodel to discover the \n",
    "        best model setting.\"\"\"\n",
    "\n",
    "        ## here we take the hypergrid and use it to construct a set of \n",
    "        # kt.HyperParameter objects\n",
    "        hypergrid = self.get_hypergrid()\n",
    "        width_hyperparam = hp.Int(\n",
    "              'units', \n",
    "              min_value=hypergrid['width']['min'], \n",
    "              max_value=hypergrid['width']['max'], \n",
    "              step=hypergrid['width']['step']\n",
    "              )\n",
    "        depth_hyperparam = hp.Int(\n",
    "              'layers', \n",
    "              min_value=hypergrid['depth']['min'], \n",
    "              max_value=hypergrid['depth']['max'], \n",
    "              step=hypergrid['depth']['step']\n",
    "              )\n",
    "        droprate_hyperparam = hp.Float(\n",
    "              'droprate', \n",
    "              min_value=hypergrid['droprate']['min'], \n",
    "              max_value=hypergrid['droprate']['max'], \n",
    "              step=hypergrid['droprate']['step']\n",
    "              )\n",
    "        activation_hyperparam = hp.Choice(\n",
    "              'activation', \n",
    "              values=hypergrid['activation']['values']\n",
    "              )\n",
    "        learning_rate_hyperparam = hp.Choice(\n",
    "              'learning_rate', \n",
    "              values=hypergrid['learning_rate']['values']\n",
    "              )\n",
    "\n",
    "        model = keras.Sequential()\n",
    "        model.add(\n",
    "              keras.layers.Input(\n",
    "              shape=self.input_shape\n",
    "              )\n",
    "            )\n",
    "        \n",
    "        ## Add a Dense-Dropout combo for each layer in the depth param\n",
    "        for layer_number in range(0,depth_hyperparam):\n",
    "              model.add(\n",
    "                    keras.layers.Dense(\n",
    "                        units=width_hyperparam,\n",
    "                        activation=activation_hyperparam,\n",
    "                        name = 'Dense_'+str(layer_number)\n",
    "                        )\n",
    "                    )\n",
    "              model.add(\n",
    "                   keras.layers.Dropout(droprate_hyperparam)\n",
    "              )\n",
    "        \n",
    "        model.add(\n",
    "                keras.layers.Dense(\n",
    "                self.n_outputs,activation=self.output_activation\n",
    "                )\n",
    "            ) \n",
    "\n",
    "        model.compile(\n",
    "            optimizer=self.optimizer_class(\n",
    "                learning_rate=learning_rate_hyperparam\n",
    "                ),\n",
    "            loss=self.loss,\n",
    "            metrics=self.metrics\n",
    "            )\n",
    "        ## This function must return an instance of a keras \n",
    "        # model that includes kt.Hyperparamater objects\n",
    "        return model \n",
    "    \n",
    "    def _hypertune(self,X_train, y_train, validation_size = 0):\n",
    "        \"\"\"This function executes the hypertuning of the\n",
    "          built hypermodel\"\"\"\n",
    "        self.build(self.hp)\n",
    "        \n",
    "        self.tuner = kt.Hyperband(\n",
    "            ## We can input an instance of the custom class here (self) \n",
    "            # and the 'build' function will be called by keras \n",
    "            # Hyperband. \n",
    "                self, \n",
    "                objective= self.objective,\n",
    "                max_epochs= self.epochs,\n",
    "                factor=self.factor,\n",
    "                hyperband_iterations=10,\n",
    "                directory=self.directory,\n",
    "                project_name=self.project_name\n",
    "                )\n",
    "        ## use validation split if validation_size > 0\n",
    "        validation_sets = None\n",
    "        if validation_size > 0:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                    X_train, \n",
    "                    y_train, \n",
    "                    test_size=validation_size\n",
    "                    )\n",
    "            validation_sets = (X_val,y_val)\n",
    "\n",
    "        self.tuner.search(\n",
    "               X_train,\n",
    "               y_train, \n",
    "               epochs=self.epochs, \n",
    "               callbacks=self.callbacks, \n",
    "               verbose=self.verbose,\n",
    "               batch_size=100,\n",
    "               use_multiprocessing=True,\n",
    "               validation_data=validation_sets\n",
    "               )\n",
    "        self.best_hyperparams=self.tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "        best_model = self.tuner.hypermodel.build(self.best_hyperparams)\n",
    "\n",
    "        return best_model\n",
    "    \n",
    "    def execute(self,X,y,validation_size = 0):\n",
    "        \"\"\"\n",
    "        Fit the model on training data.\n",
    "\n",
    "        Args:\n",
    "            - X (_type_): predictors, ndarray\n",
    "            - y (_type_): dependant, ndarray or 1-d array\n",
    "            - validation_size (int, optional): If > 0, tuner will \n",
    "            record validation performance of the objective. Set \n",
    "            objective to 'val_<metric name>' to select best model \n",
    "            on validation data. Defaults to 0.\n",
    "\n",
    "        Raises:\n",
    "            - e: If errors occur during fitting, a 'final' clause is \n",
    "            invoked to ensure the tuning directory is still deleted. \n",
    "        \"\"\"\n",
    "        ## Use finally clause to ensure the tuning dir is deleted even \n",
    "        # if there is an error.This failsafe is used because 'leftover' \n",
    "        # tuning directories from previous fittings can cause errors.\n",
    "        try:\n",
    "            self.model = self._hypertune(X,y,validation_size)\n",
    "            self.history = self.model.fit(\n",
    "                X,\n",
    "                y, \n",
    "                batch_size=self.batch_size, \n",
    "                epochs=self.epochs,\n",
    "                callbacks=self.callbacks, \n",
    "                verbose = self.verbose)\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "        finally:\n",
    "            try:\n",
    "                  shutil.rmtree(self.directory)\n",
    "            except FileNotFoundError as e:\n",
    "                 pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrating Hypermodel Class\n",
    "You may skip reading the detail of how the `ANNHypermodel` class works and instead just review the following section to see a demonstration use case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 4393] Das im Analysepunktpuffer vorhandene Kennzeichen ist ungltig: 'kt-dir\\\\kt_hyperband\\\\trial_0193'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m\n\u001b[0;32m      1\u001b[0m hypergrid \u001b[39m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m:{\n\u001b[0;32m      3\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m                 }\n\u001b[0;32m     20\u001b[0m             }\n\u001b[0;32m     22\u001b[0m model \u001b[39m=\u001b[39m ANNHyperModel(\n\u001b[0;32m     23\u001b[0m     input_shape \u001b[39m=\u001b[39minput_shape,\n\u001b[0;32m     24\u001b[0m     n_outputs \u001b[39m=\u001b[39m n_classes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m         ]\n\u001b[0;32m     36\u001b[0m     )\n\u001b[1;32m---> 38\u001b[0m model\u001b[39m.\u001b[39;49mexecute(X_train,y_train,validation_size\u001b[39m=\u001b[39;49m\u001b[39m.2\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[7], line 289\u001b[0m, in \u001b[0;36mANNHyperModel.execute\u001b[1;34m(self, X, y, validation_size)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m           shutil\u001b[39m.\u001b[39;49mrmtree(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdirectory)\n\u001b[0;32m    290\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    291\u001b[0m          \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:749\u001b[0m, in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39m# can't continue even if onerror hook returns\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m \u001b[39mreturn\u001b[39;00m _rmtree_unsafe(path, onerror)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:614\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    612\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    613\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    615\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:614\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    612\u001b[0m         onerror(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mislink, fullname, sys\u001b[39m.\u001b[39mexc_info())\n\u001b[0;32m    613\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     _rmtree_unsafe(fullname, onerror)\n\u001b[0;32m    615\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    616\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:600\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    598\u001b[0m         entries \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(scandir_it)\n\u001b[0;32m    599\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m--> 600\u001b[0m     onerror(os\u001b[39m.\u001b[39;49mscandir, path, sys\u001b[39m.\u001b[39;49mexc_info())\n\u001b[0;32m    601\u001b[0m     entries \u001b[39m=\u001b[39m []\n\u001b[0;32m    602\u001b[0m \u001b[39mfor\u001b[39;00m entry \u001b[39min\u001b[39;00m entries:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\shutil.py:597\u001b[0m, in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_rmtree_unsafe\u001b[39m(path, onerror):\n\u001b[0;32m    596\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 597\u001b[0m         \u001b[39mwith\u001b[39;00m os\u001b[39m.\u001b[39;49mscandir(path) \u001b[39mas\u001b[39;00m scandir_it:\n\u001b[0;32m    598\u001b[0m             entries \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(scandir_it)\n\u001b[0;32m    599\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 4393] Das im Analysepunktpuffer vorhandene Kennzeichen ist ungltig: 'kt-dir\\\\kt_hyperband\\\\trial_0193'"
     ]
    }
   ],
   "source": [
    "hypergrid = {\n",
    "                'width':{\n",
    "                    'min':16,\n",
    "                    'max':64,\n",
    "                    'step':12\n",
    "                    },\n",
    "                'depth':{\n",
    "                    'min':1,\n",
    "                    'max':4,\n",
    "                    'step':1\n",
    "                    },\n",
    "                'droprate':{\n",
    "                    'min':0.1,\n",
    "                    'max':0.7,\n",
    "                    'step':0.2\n",
    "                    },\n",
    "                'activation':{\n",
    "                    'values':['relu']\n",
    "                }\n",
    "            }\n",
    "\n",
    "model = ANNHyperModel(\n",
    "    input_shape =input_shape,\n",
    "    n_outputs = n_classes,\n",
    "    epochs = 150,\n",
    "    loss = keras.losses.BinaryCrossentropy(),\n",
    "    hypergrid = hypergrid,\n",
    "    objective=\"val_accuracy\",\n",
    "    directory = 'kt-dir',\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            monitor='loss', \n",
    "            patience=5\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "model.execute(X_train,y_train,validation_size=.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making predictions\n",
    "Now that we have our fitted model, let us evaluate its predictive power. In this context, we're going to use accuracy as our evaluation metric. Accuracy is the portion of classifications which were correct. Typically, this metric is suitable when there are roughly equal numbers of classes in the data and we consider any kind of misclassification as equally undesirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 4ms/step\n",
      "Accuracy of this model is: 98.00%\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test.argmax(axis=1),y_pred)\n",
    "print(\"Accuracy of this model is: \"+\"{:1.2f}\".format(accuracy*100)+\"%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success!! The best model discovered by our hypertuner was 98% accurate, a big improvement over 70% in our basic model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting history\n",
    "The history parameter helps us monitor how effectively the fitting process converged on the optimum accuracy. We can construct a timeline which shows us how the accuracy metric improved at each epoch as the optimizer progressively improved the weightings. Note that once a point of diminishing return is reached, the EarlyStopping callback function will conclude the fitting process early to save computation time, which is why we cannot see the requested 150 epochs. \n",
    "\n",
    "Practitioners should expect to see a steady increase in performance over epochs. A history showing no improvement over epochs would indicate a problem with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Epochs', ylabel='Accuracy'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(\n",
    "    model.history.history['accuracy']\n",
    "    ).plot(\n",
    "        xlabel='Epochs',\n",
    "        ylabel=\"Accuracy\"\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What were the best hyperparameters?\n",
    "The `best_hyperparams` property will tell us which of the parameters we trialed was best. Note that some of the 'tuner' parameters are internal considerations for the tuner and not important to us for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units': 64,\n",
       " 'layers': 4,\n",
       " 'droprate': 0.1,\n",
       " 'activation': 'relu',\n",
       " 'learning_rate': 0.01,\n",
       " 'tuner/epochs': 6,\n",
       " 'tuner/initial_epoch': 2,\n",
       " 'tuner/bracket': 4,\n",
       " 'tuner/round': 1,\n",
       " 'tuner/trial_id': '0031'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_hyperparams.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does our model look like?\n",
    "We can use the `summary()` function to print out a basic summary fo our model's architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_0 (Dense)             (None, 64)                320       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " Dense_3 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,995\n",
      "Trainable params: 12,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance\n",
    "We know the model was 96% accurate, but let us examine more deeply how well it classified each species. We shall use a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Confusion Matrix')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.rc('figure', figsize=(5,5))\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(\n",
    "        y_test.argmax(axis=1), \n",
    "        y_pred)\n",
    "        ).plot(\n",
    "            colorbar=False\n",
    "            )\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the model\n",
    "Most consumers of machine learning models want to know *why* a model gives the output it does. This can be a difficult task for a complex neural network, with many interactions nested in the algorithm. In a linear model, each feature has a positive or negative impact on the outcome. In an interactive model, the direction and size of effect is conditional and not so simple to describe. \n",
    "A common method for explaining why models output as they do is 'permutational feature importance' in which we calculate the loss of accuracy in a model when it does not have a given feature available. However, this is a lot of repeated computation when you have many features and ANNs can be slow for this. \n",
    "\n",
    "Another approach is [SHapley Additive exPlanations (SHAP)](https://shap.readthedocs.io/en/latest/), which uses an algorithm based on game-theory as well as many computational optimizations, to efficiently calculate 'SHAP values' for an ANN. Here we shall demonstrate SHAP. A larger SHAP value indicates the feature is more influential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 252, in grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 378, in custom_grad\n        out = op_handlers[type_name](self, op, *grads) # we cut off the shap_ prefex before the lookup\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 562, in handler\n        var = explainer._variable_inputs(op)\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 222, in _variable_inputs\n        out = np.zeros(len(op.inputs), dtype=np.bool)\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\numpy\\__init__.py\", line 305, in __getattr__\n        raise AttributeError(__former_attrs__[attr])\n\n    AttributeError: module 'numpy' has no attribute 'bool'.\n    `np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n    The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n        https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m deep_explain \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mDeepExplainer(model\u001b[39m.\u001b[39mmodel, X_test)\n\u001b[1;32m----> 2\u001b[0m deep_shap_values\u001b[39m=\u001b[39m deep_explain\u001b[39m.\u001b[39;49mshap_values(X_test[\u001b[39m1\u001b[39;49m:\u001b[39m5\u001b[39;49m])\n\u001b[0;32m      4\u001b[0m matplotlib\u001b[39m.\u001b[39mrc(\u001b[39m'\u001b[39m\u001b[39mfigure\u001b[39m\u001b[39m'\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m10\u001b[39m))\n\u001b[0;32m      5\u001b[0m pd\u001b[39m.\u001b[39mSeries(\n\u001b[0;32m      6\u001b[0m     deep_shap_values[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m      7\u001b[0m     index\u001b[39m=\u001b[39mfeature_names\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m         title\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSHAP Values\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     11\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\__init__.py:124\u001b[0m, in \u001b[0;36mDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshap_values\u001b[39m(\u001b[39mself\u001b[39m, X, ranked_outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, output_rank_order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, check_additivity\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     91\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Return approximate SHAP values for the model applied to the data given by X.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39m        were chosen as \"top\".\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplainer\u001b[39m.\u001b[39;49mshap_values(X, ranked_outputs, output_rank_order, check_additivity\u001b[39m=\u001b[39;49mcheck_additivity)\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:312\u001b[0m, in \u001b[0;36mTFDeep.shap_values\u001b[1;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39m# run attribution computation graph\u001b[39;00m\n\u001b[0;32m    311\u001b[0m feature_ind \u001b[39m=\u001b[39m model_output_ranks[j,i]\n\u001b[1;32m--> 312\u001b[0m sample_phis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mphi_symbolic(feature_ind), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_inputs, joint_input)\n\u001b[0;32m    314\u001b[0m \u001b[39m# assign the attributions to the right part of the output arrays\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X)):\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:372\u001b[0m, in \u001b[0;36mTFDeep.run\u001b[1;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[0;32m    369\u001b[0m         tf_execute\u001b[39m.\u001b[39mrecord_gradient \u001b[39m=\u001b[39m tf_backprop\u001b[39m.\u001b[39mrecord_gradient\n\u001b[0;32m    371\u001b[0m     \u001b[39mreturn\u001b[39;00m final_out\n\u001b[1;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_with_overridden_gradients(anon)\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:408\u001b[0m, in \u001b[0;36mTFDeep.execute_with_overridden_gradients\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[39m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 408\u001b[0m     out \u001b[39m=\u001b[39m f()\n\u001b[0;32m    409\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     \u001b[39m# reinstate the backpropagatable check\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tf_gradients_impl, \u001b[39m\"\u001b[39m\u001b[39m_IsBackpropagatable\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:365\u001b[0m, in \u001b[0;36mTFDeep.run.<locals>.anon\u001b[1;34m()\u001b[0m\n\u001b[0;32m    363\u001b[0m     v \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconstant(data, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_inputs[i]\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    364\u001b[0m     inputs\u001b[39m.\u001b[39mappend(v)\n\u001b[1;32m--> 365\u001b[0m final_out \u001b[39m=\u001b[39m out(inputs)\n\u001b[0;32m    366\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     tf_execute\u001b[39m.\u001b[39mrecord_gradient \u001b[39m=\u001b[39m tf_backprop\u001b[39m.\u001b[39m_record_gradient\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filerv794tqf.py:34\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__grad_graph\u001b[1;34m(shap_rAnD)\u001b[0m\n\u001b[0;32m     32\u001b[0m     ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mmulti_output, if_body, else_body, get_state, set_state, (\u001b[39m'\u001b[39m\u001b[39mout\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m_init_between_tensors, (ag__\u001b[39m.\u001b[39mld(out)\u001b[39m.\u001b[39mop, ag__\u001b[39m.\u001b[39mld(shap_rAnD)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 34\u001b[0m x_grad \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tape)\u001b[39m.\u001b[39;49mgradient, (ag__\u001b[39m.\u001b[39;49mld(out), ag__\u001b[39m.\u001b[39;49mld(shap_rAnD)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[0;32m     35\u001b[0m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mset_learning_phase, (ag__\u001b[39m.\u001b[39mld(phase),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     36\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:378\u001b[0m, in \u001b[0;36mTFDeep.custom_grad\u001b[1;34m(self, op, *grads)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Passes a gradient op creation request to the correct handler.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m type_name \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39mtype[\u001b[39m5\u001b[39m:] \u001b[39mif\u001b[39;00m op\u001b[39m.\u001b[39mtype\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mshap_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m op\u001b[39m.\u001b[39mtype\n\u001b[1;32m--> 378\u001b[0m out \u001b[39m=\u001b[39m op_handlers[type_name](\u001b[39mself\u001b[39;49m, op, \u001b[39m*\u001b[39;49mgrads) \u001b[39m# we cut off the shap_ prefex before the lookup\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:562\u001b[0m, in \u001b[0;36mlinearity_1d_nonlinearity_2d.<locals>.handler\u001b[1;34m(explainer, op, *grads)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhandler\u001b[39m(explainer, op, \u001b[39m*\u001b[39mgrads):\n\u001b[1;32m--> 562\u001b[0m     var \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39;49m_variable_inputs(op)\n\u001b[0;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m var[input_ind0] \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m var[input_ind1]:\n\u001b[0;32m    564\u001b[0m         \u001b[39mreturn\u001b[39;00m linearity_1d_handler(input_ind0, explainer, op, \u001b[39m*\u001b[39mgrads)\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py:222\u001b[0m, in \u001b[0;36mTFDeep._variable_inputs\u001b[1;34m(self, op)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Return which inputs of this operation are variable (i.e. depend on the model inputs).\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39mif\u001b[39;00m op \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vinputs:\n\u001b[1;32m--> 222\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(op\u001b[39m.\u001b[39minputs), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mbool)\n\u001b[0;32m    223\u001b[0m     \u001b[39mfor\u001b[39;00m i,t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(op\u001b[39m.\u001b[39minputs):\n\u001b[0;32m    224\u001b[0m         out[i] \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mname \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbetween_tensors\n",
      "File \u001b[1;32mc:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\numpy\\__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    301\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    307\u001b[0m \u001b[39m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[39m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[39m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 252, in grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 378, in custom_grad\n        out = op_handlers[type_name](self, op, *grads) # we cut off the shap_ prefex before the lookup\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 562, in handler\n        var = explainer._variable_inputs(op)\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\shap\\explainers\\_deep\\deep_tf.py\", line 222, in _variable_inputs\n        out = np.zeros(len(op.inputs), dtype=np.bool)\n    File \"c:\\Users\\jocollin\\OneDrive - uni-mannheim.de\\repos\\DMU\\DMU_NN_TUTORIAL\\venv\\lib\\site-packages\\numpy\\__init__.py\", line 305, in __getattr__\n        raise AttributeError(__former_attrs__[attr])\n\n    AttributeError: module 'numpy' has no attribute 'bool'.\n    `np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n    The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n        https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    }
   ],
   "source": [
    "deep_explain = shap.DeepExplainer(model.model, X_test)\n",
    "deep_shap_values= deep_explain.shap_values(X_test[1:5])\n",
    "\n",
    "matplotlib.rc('figure', figsize=(10,10))\n",
    "pd.Series(\n",
    "    deep_shap_values[0][0],\n",
    "    index=feature_names\n",
    "    ).sort_values(\n",
    "    ).plot.barh(\n",
    "        title='SHAP Values'\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that petal length and width are the most important factors when trying to classify a flower. Sepal width is also important, but sepal length is relatively inconsequential. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Keras Models\n",
    "Often, once we have developed our model, we want to deploy it elsewhere so it can used in some other application. \n",
    "Keras offers a method for saving the model as a directory of files, which can then be sent anywhere you desire. \n",
    "That directory can then be loaded in python runtime to instantiate a replica of the fitted model. It is ready to make predictions right after loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of this simple model is: 96.00%\n"
     ]
    }
   ],
   "source": [
    "model.model.save('keras_model')\n",
    "loaded_model = keras.models.load_model('keras_model')\n",
    "\n",
    "loaded_model_y_preds = loaded_model.predict(X_test) >= .5\n",
    "\n",
    "loaded_model_accuracy = accuracy_score(\n",
    "    y_test,\n",
    "    y_pred = loaded_model_y_preds\n",
    "    )\n",
    "print(\n",
    "    \"Accuracy of this simple model is: \"+\n",
    "    \"{:1.2f}\".format(accuracy*100)+\"%\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other use cases\n",
    "In case you want to reuse this code, but for some other use case, like binary classification or regression, we demonstrate how the same class can easily accomplish those as well. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of this model is: 95.21%\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_breast_cancer(\n",
    "    return_X_y=False, as_frame=False\n",
    "    )\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "y = np_utils.to_categorical(y)\n",
    "n_classes = len(dataset.target_names) \n",
    "input_shape = len(dataset.feature_names)\n",
    "feature_names = dataset.feature_names \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.33, \n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "model = ANNHyperModel(\n",
    "    input_shape =input_shape,\n",
    "    n_outputs = n_classes,\n",
    "    epochs = 150,\n",
    "    loss = keras.losses.BinaryCrossentropy(),\n",
    "    hypergrid = hypergrid,\n",
    "    objective=\"val_accuracy\",\n",
    "    directory = 'kt-dir',\n",
    "    callbacks=[EarlyStopping(monitor='loss', patience=5)]\n",
    "    )\n",
    "\n",
    "model.execute(X_train,y_train,validation_size=.2)\n",
    "\n",
    "y_pred = model.model.predict(X_test).argmax(axis=1)\n",
    "accuracy = accuracy_score(y_test.argmax(axis=1),y_pred)\n",
    "print(\n",
    "    \"Accuracy of this model is: \"+\n",
    "    \"{:1.2f}\".format(accuracy*100)+\"%\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of this model is: 4429.89\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = datasets.load_diabetes(return_X_y=False)\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "input_shape = len(dataset.feature_names)\n",
    "feature_names = dataset.feature_names \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.33, \n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "model = ANNHyperModel(\n",
    "    input_shape =input_shape,\n",
    "    ## we are predicting one continuous variable, so we want \n",
    "    # only one output node\n",
    "    n_outputs = 1, \n",
    "    epochs = 150,\n",
    "    loss = keras.losses.MeanAbsoluteError(),\n",
    "    output_activation=None,\n",
    "    hypergrid = hypergrid,\n",
    "    metrics=[\n",
    "            tf.keras.metrics.MeanSquaredError(\n",
    "            name=\"mean_absolute_error\", \n",
    "            dtype=None\n",
    "            )\n",
    "        ],\n",
    "    objective=\"val_mean_absolute_error\",\n",
    "    directory = 'kt-dir',\n",
    "    callbacks=[EarlyStopping(monitor='loss', patience=5)]\n",
    "    )\n",
    "\n",
    "model.execute(X_train,y_train,validation_size=.2)\n",
    "\n",
    "## use flatten because the single output node outputs a \n",
    "# series of vectors of 1 item each. Flatten makes \n",
    "# 1d array instead\n",
    "y_pred = model.model.predict(X_test).flatten() \n",
    "\n",
    "error = mean_absolute_error(y_test,y_pred)\n",
    "print(\"MAE of this model is: \"+\"{:1.2f}\".format(error*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This article provided a hand-on demonstration of how to implement ANNs for your own context. By following the code in this article, you should e able to adapt the reusable `ANNHypermodel` class to your own project. ANNs are a powerful machine learning algorithm, and can be applied to many different problems. With a basic understanding of how to implement ANNs, you can also start to progress to learn more advanced types of neural networks, including Recurrent Neural Networks and Convolutional Neural Networks. \n",
    "\n",
    "### Further reading\n",
    "- [This video tutorial series on the theory of ANNs.](https://www.youtube.com/watch?v=CqOfi41LfDw&ab_channel=StatQuestwithJoshStarmer)\n",
    "- [The tensorflow playground](https://playground.tensorflow.org), a fun visual aid for intuitively understanding how neural networks function. \n",
    "- [The documentation for Tensorflow.](https://www.tensorflow.org/)\n",
    "- [The documentation for SHAP.](https://shap.readthedocs.io/en/latest/)\n",
    "\n",
    "### About the author\n",
    "[John 'Jack' Collins](https://www.mzes.uni-mannheim.de/d7/en/profiles/john-james-collins) is a PhD candidate at the Mannheimer Zentrum fr Europische Sozialforschung (MZES). Jack holds a Master's in  Data Science and his research focuses on applying machine learning to survey methodology. Before coming to Mannheim University for PhD, Jack was an IT consultant. You can find Jack on [LinkedIn](https://www.linkedin.com/in/jack-collins-595a53115/) and through his [website](https://jackcollins91.github.io/jcowebsite/). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication information\n",
    "Python libraries are updated from time to time. Sometimes, when you import code from another project, it can suddenly fail because of version differences. To help with replicability, here's a dump of the version of every module in the environment as of when this code ran successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f909c0dcb1f91e5e5c56d5a5ffbeb389c696e04e7391234a5a7c6e7b702cb751"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
